import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import torchvision.models as models
from PIL import Image, ImageOps
from flask import Flask, request, jsonify

# Segmentation éœ€è¦çš„å‡½å¼åº«
import cv2
import numpy as np
import base64
from ultralytics import YOLO
import datetime 

app = Flask(__name__)

# --- è¨­å®šè·¯å¾‘ ---
CLS_MODEL_PATH = "models/model_v3_aug.pth"
SEG_MODEL_PATH = "models/yolo11n-seg.pt"
UPLOAD_FOLDER = "uploads"
DEBUG_FOLDER = "debug_images"  
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)
if not os.path.exists(DEBUG_FOLDER):
    os.makedirs(DEBUG_FOLDER)

# ==========================================
# ğŸ¯ è¨­å®šè£åˆ‡ç¯„åœ (é©ç”¨æ–¼ Classification èˆ‡ Segmentation)
# ==========================================
# æ ¼å¼: (å·¦ä¸Šx, å·¦ä¸Šy, å³ä¸‹x, å³ä¸‹y)
FIXED_CROP_BOX = (200, 500, 900, 1100)

# --- é¡åˆ¥å®šç¾© ---
CLASSES = [
    'ä½•é¦–çƒ', 'å±±è—¥', 'å·èŠ', 'æœ¨é¦™', 'ç†Ÿåœ°', 
    'ç”˜è‰', 'ç™½æœ®ç‰‡', 'ç™½èŠ·', 'ç´…è€†', 'ç¾Šå¥¶é ­', 
    'èŒ¯è‹“', 'èªæœ®', 'è’¼æœ®ç‰‡', 'é™³çš®', 'é»ƒè€†'
]

LABEL_TO_KEY_MAP = {
    'ä½•é¦–çƒ': 'Fallopia multiflora',
    'å±±è—¥': 'Dioscorea polystachya Turcz',
    'å·èŠ': 'Ligusticum chuanxiong Hort',
    'æœ¨é¦™': 'Radix Aucklandiae',
    'ç†Ÿåœ°': 'Prepared Rehmannia Root',
    'ç”˜è‰': 'Licorice',
    'ç™½æœ®ç‰‡': 'Baizhu Slices',
    'ç™½èŠ·': 'Dahurian Angelica',
    'ç´…è€†': 'Hedysarum Root',
    'ç¾Šå¥¶é ­': 'Taiwan Ficus',
    'èŒ¯è‹“': 'Poria',
    'èªæœ®': 'Zedoary Rhizome',
    'è’¼æœ®ç‰‡': 'Atractylodes Rhizome',
    'é™³çš®': 'Citri Reticulatae Pericarpium',
    'é»ƒè€†': 'Astragalus membranaceus'
}

# --- è¼‰å…¥æ¨¡å‹ ---
def load_classification_model():
    print("æ­£åœ¨è¼‰å…¥ Classification æ¨¡å‹...")
    model = models.efficientnet_b0(weights=None)
    in_features = model.classifier[1].in_features
    model.classifier = nn.Sequential(nn.Dropout(p=0.5), nn.Linear(in_features, len(CLASSES)))
    try:
        checkpoint = torch.load(CLS_MODEL_PATH, map_location=DEVICE)
        state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint
        model.load_state_dict(state_dict)
        model.to(DEVICE)
        model.eval()
        print("âœ… Classification æ¨¡å‹è¼‰å…¥æˆåŠŸï¼")
        return model
    except Exception as e:
        print(f"âŒ Classification æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        return None 

def load_segmentation_model():
    print(f"æ­£åœ¨è¼‰å…¥ Segmentation æ¨¡å‹...")
    try:
        model = YOLO(SEG_MODEL_PATH)
        print("âœ… Segmentation æ¨¡å‹è¼‰å…¥æˆåŠŸï¼")
        return model
    except Exception as e:
        print(f"âŒ Segmentation æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        return None

cls_model = load_classification_model()
seg_model = load_segmentation_model()

# --- é è™•ç† ---
preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

@app.route('/upload', methods=['POST'])
def predict_api():
    if 'file' not in request.files:
        return jsonify({'status': 'error', 'message': 'æœªå‚³é€æª”æ¡ˆ'}), 400
    
    file = request.files['file']
    mode = request.form.get('mode', 'Classification')
    
    try:
        # 1. é–‹å•Ÿåœ–ç‰‡ä¸¦ä¿®æ­£è½‰å‘
        image = Image.open(file)
        image = ImageOps.exif_transpose(image)
        image = image.convert('RGB')
        
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

        # ==========================================
        # âœ‚ï¸ å…¨åŸŸè£åˆ‡ï¼šæ‰€æœ‰æ¨¡å¼éƒ½ä½¿ç”¨é€™å¼µè£åˆ‡å¾Œçš„åœ–
        # ==========================================
        print(f"âœ‚ï¸ æ­£åœ¨è£åˆ‡åœ–ç‰‡ï¼Œç¯„åœ: {FIXED_CROP_BOX}")
        cropped_image = image.crop(FIXED_CROP_BOX)
        
        # å„²å­˜è£åˆ‡å¾Œçš„åœ–ç‰‡ (è«‹å‹™å¿…å»è³‡æ–™å¤¾æª¢æŸ¥é€™å¼µåœ–ï¼)
        debug_path_crop = os.path.join(DEBUG_FOLDER, f"{timestamp}_{mode}_CROP.jpg")
        cropped_image.save(debug_path_crop)
        print(f"ğŸ“¸ å·²å„²å­˜è£åˆ‡åœ–ç‰‡: {debug_path_crop}")

        # --- åˆ†æ”¯ 1: Classification (ä½¿ç”¨è£åˆ‡åœ–) ---
        if mode == 'Classification':
            if cls_model is None: return jsonify({'status': 'error', 'message': 'æ¨¡å‹æœªè¼‰å…¥'}), 500

            # ä½¿ç”¨ã€Œè£åˆ‡å¾Œçš„åœ–ç‰‡ã€é€²è¡Œé è™•ç†
            input_tensor = preprocess(cropped_image).unsqueeze(0).to(DEVICE)

            with torch.no_grad():
                output = cls_model(input_tensor)        
                probs = F.softmax(output, dim=1)
                top_p, top_class = probs.topk(1, dim=1)
                
                chinese_label = CLASSES[top_class.item()]
                app_herb_id = LABEL_TO_KEY_MAP.get(chinese_label, "Unknown")
                
                print(f"ğŸ” Classification é æ¸¬ (è£åˆ‡å¾Œ): {chinese_label} ({top_p.item():.2%})")

            return jsonify({
                'status': 'success',
                'result': {
                    'herb_id': app_herb_id,    
                    'confidence': top_p.item(),
                    'note': 'Prediction based on cropped area'
                }
            })

        # --- åˆ†æ”¯ 2: Segmentation (ä½¿ç”¨è£åˆ‡åœ–) ---
        elif mode == 'Segmentation':
            if seg_model is None: return jsonify({'status': 'error', 'message': 'æ¨¡å‹æœªè¼‰å…¥'}), 500

            # ä½¿ç”¨ã€Œè£åˆ‡å¾Œçš„åœ–ç‰‡ã€ä¸Ÿçµ¦ YOLO
            results = seg_model.predict(source=cropped_image, conf=0.25)
            
            if len(results) == 0 or len(results[0].boxes) == 0:
                print("âš ï¸ åˆ†å‰²æ¨¡å¼ï¼šè£åˆ‡ç¯„åœå…§æœªåµæ¸¬åˆ°ç‰©ä»¶")
                annotated_frame = np.array(cropped_image) 
                annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR)
            else:
                result = results[0]
                print(f"âœ‚ï¸ åˆ†å‰²æ¨¡å¼ï¼šåœ¨è£åˆ‡ç¯„åœå…§åµæ¸¬åˆ° {len(result.boxes)} å€‹ç‰©ä»¶")
                annotated_frame = result.plot()

            # è½‰ Base64
            retval, buffer = cv2.imencode('.jpg', annotated_frame)
            if retval:
                base64_string = base64.b64encode(buffer).decode('utf-8')
                return jsonify({
                    'status': 'success',
                    'result': {
                        'segmentation_image_base64': base64_string
                    }
                })
            else:
                return jsonify({'status': 'error', 'message': 'åœ–ç‰‡ç·¨ç¢¼å¤±æ•—'}), 500

    except Exception as e:
        print(f"âŒ Server éŒ¯èª¤: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

if __name__ == "__main__":
    app.run(host='0.0.0.0', port=5001, debug=False)