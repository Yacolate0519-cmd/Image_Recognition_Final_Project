from PIL import Image, ImageOps, ImageDraw, ImageFont
import torchvision.transforms as transforms
import torchvision.models as models
import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import math

# --- åƒæ•¸è¨­å®š ---
# è«‹æ›¿æ›æˆä½ è¦æ¸¬è©¦çš„åœ–ç‰‡è·¯å¾‘
img_path = r"/Users/yacolate0519/Desktop/åœ–åƒè­˜åˆ¥æœŸæœ«å°ˆæ¡ˆ/debug_images/IMG_1118.jpeg" 
model_path = r"/Users/yacolate0519/Desktop/åœ–åƒè­˜åˆ¥æœŸæœ«å°ˆæ¡ˆ/models/Half Data.pth"

classes = ['å±±è—¥', 'å·èŠ', 'æœ¨é¦™', 'ç†Ÿåœ°', 'ç”˜è‰', 'ç™½èŠ·', 'ç´…è€†', 'èªæœ®', 'è’¼æœ®', 'é™³çš®', 'é»ƒè€†']
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def get_font(size=20):
    """è‡ªå‹•å°‹æ‰¾ç³»çµ±å¯ç”¨çš„ä¸­æ–‡å­—é«”"""
    mac_fonts = [
        "/System/Library/Fonts/STHeiti Light.ttc",
        "/System/Library/Fonts/PingFang.ttc",
        "/Library/Fonts/Arial Unicode.ttf"
    ]
    for font_path in mac_fonts:
        if os.path.exists(font_path):
            try: return ImageFont.truetype(font_path, size)
            except: continue
    print("âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ°ä¸­æ–‡å­—é«”")
    return ImageFont.load_default()

def add_frame_and_text(img, text, border_width=2, border_color='black', text_color='blue'):
    final_img = ImageOps.expand(img, border=border_width, fill=border_color)
    draw = ImageDraw.Draw(final_img)
    font = get_font(size=20) 
    w, h = final_img.size
    try:
        left, top, right, bottom = draw.textbbox((0, 0), text, font=font)
        text_w, text_h = right - left, bottom - top
    except AttributeError:
        text_w, text_h = draw.textsize(text, font=font)
    
    draw.text(((w - text_w) / 2, (h - text_h) / 2), text, font=font, fill=text_color, stroke_width=2, stroke_fill='white')
    return final_img

def display_image_list(images, cols=4):
    if not images: return
    w, h = images[0].size
    rows = math.ceil(len(images) / cols)
    grid = Image.new('RGB', (w * cols, h * rows), (255, 255, 255))
    for i, img in enumerate(images):
        grid.paste(img, ((i % cols) * w, (i // cols) * h))
    print("æ­£åœ¨é–‹å•Ÿåœ–ç‰‡è¦–çª—...")
    grid.show()

def load_model():
    try:
        model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.IMAGENET1K_V1)
    except:
        model = models.efficientnet_b2(weights=None)
    for param in model.parameters(): param.requires_grad = False
    model.classifier = nn.Sequential(nn.Dropout(p=0.5), nn.Linear(model.classifier[1].in_features, len(classes)))
    
    checkpoint = torch.load(model_path, map_location=device)
    state = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint
    model.load_state_dict(state)
    model.to(device).eval()
    print("æ¨¡å‹æ¬Šé‡è¼‰å…¥æˆåŠŸï¼")
    return model

preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

def split_img(img_obj, splits):
    # ä¿®æ”¹ï¼šç§»é™¤äº†è£åˆ‡ç‰¹å®šå€åŸŸçš„æ­¥é©Ÿï¼Œç›´æ¥ä½¿ç”¨æ•´å¼µåœ–
    img = img_obj 
    
    result = []
    img_w, img_h = img.size
    step_w, step_h = int(img_w / splits), int(img_h / splits)
    
    for row in range(splits):
        for col in range(splits):
            left, upper = col * step_w, row * step_h
            right = left + step_w if col < splits - 1 else img_w
            lower = upper + step_h if row < splits - 1 else img_h
            result.append(img.crop((left, upper, right, lower)))
    return result

def predict_image(image, model):
    input_tensor = preprocess(image).unsqueeze(0).to(device)
    with torch.no_grad():
        output = model(input_tensor)
        probs = F.softmax(output, dim=1)
        top_p, top_class = probs.topk(1, dim=1)
    return top_class[0].item(), top_p[0].item() * 100

if __name__ == "__main__":
    if not os.path.exists(img_path) or not os.path.exists(model_path):
        print("âŒ æ‰¾ä¸åˆ°åœ–ç‰‡æˆ–æ¨¡å‹æª”æ¡ˆ")
    else:
        model = load_model()
        splits = 4 # åˆ‡æˆ 4x4 = 16 å¼µ

        print(f"\n--- é–‹å§‹è™•ç†æµç¨‹ ---")
        
        # 1. è®€å–åŸå§‹åœ–ç‰‡
        print(f"è®€å–åœ–ç‰‡: {img_path}")
        original_img = Image.open(img_path)
        original_img = ImageOps.exif_transpose(original_img) # ä¿®æ­£æ–¹å‘

        # --- 2. é€²è¡Œåˆ‡å¡Š (å…¨åœ–åˆ‡å‰²) ---
        print(f"\næ­£åœ¨é€²è¡Œå…¨åœ–ç¶²æ ¼åˆ‡å‰²...")
        imgs = split_img(original_img, splits)
        
        if imgs:
            imgs_text = []
            vote_box = {c: 0.0 for c in classes}

            print(f"å…±æœ‰ {len(imgs)} å¼µå°åœ–é€²è¡ŒæŠ•ç¥¨...\n")

            # --- 3. é€å¼µé æ¸¬ä¸¦ç´¯åŠ åˆ†æ•¸ ---
            for i, img in enumerate(imgs):
                idx, conf = predict_image(img, model)
                label = classes[idx]
                vote_box[label] += conf
                
                display_text = f"{label}\n{int(conf)}%"
                imgs_text.append(add_frame_and_text(img, display_text))
            
            print("--- åˆ‡å‰²åœ–è¾¨è­˜å®Œæˆï¼Œé–‹å•Ÿçµæœåœ– ---")
            display_image_list(imgs_text, cols=splits)
            
            # --- 4. çµç®—ç¸½åˆ† ---
            sorted_votes = sorted(vote_box.items(), key=lambda x: x[1], reverse=True)
            
            print("\n" + "="*30)
            print("ğŸ“Š æœ€çµ‚ä¿¡å¿ƒåº¦çµ±è¨ˆ (ç¸½åˆ†æ’å)")
            print("="*30)
            
            for rank, (herb_name, total_score) in enumerate(sorted_votes[:3]):
                avg_score = total_score / (splits * splits)
                print(f"ç¬¬ {rank+1} å: ã€{herb_name}ã€‘")
                print(f"   ç¸½ç´¯ç©ä¿¡å¿ƒåº¦: {total_score:.2f}")
                print(f"   å¹³å‡å–®å¼µä¿¡å¿ƒ: {avg_score:.2f}%")
                print("-" * 20)
                
            winner = sorted_votes[0][0]
            print(f"\nğŸ† åˆ¤å®šçµæœ: é€™å¼µåœ–ç‰‡æ˜¯ [{winner}]")
            print("="*30)
            
        else:
            print("âŒ åœ–ç‰‡è™•ç†å¤±æ•—")